{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c1a6638-5eef-4566-b9a3-5acdce76571f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello<|im>assistant>Hello<|assistant>assistantHello<assistantassistantHello<assistantassistantassistantHello<assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"you are an AI agent.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello.\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=128,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "output = tokenizer.decode(response, skip_special_tokens=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c03f98-fb0c-44ea-9662-9070e0931483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c47551c2a14fa3ad84a0ef30317b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/root/autodl-tmp/models/mistral_7b_instruct_50_dpo\", trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"/root/autodl-tmp/models/mistral_7b_instruct_50_dpo\", trust_remote_code=True).to('cuda:0')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/autodl-tmp/models/gemma_2b_it_50_dpo\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/root/autodl-tmp/models/gemma_2b_it_50_dpo\", trust_remote_code=True).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51a37a61-2534-4c62-a716-511703f0d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1(str1, str2):\n",
    "    set1, set2 = set(str1), set(str2)\n",
    "    intersection = len(set1 & set2)\n",
    "    if not intersection:\n",
    "        return 0\n",
    "    precision = intersection / len(set1)\n",
    "    recall = intersection / len(set2)\n",
    "    return 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96e91769-17d1-4d26-878a-a5e1a9ce6911",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing test_website dpo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 604/1030 [10:06<12:16,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTION_HISTORY: CLICK\n",
      "ID: 50887\n",
      "ID: 508877\n",
      "ACTION_HISTORY: CLICK\n",
      "ID: 48887\n",
      "ACTION_HISTORY: CLICK\n",
      "ID: 51329\n",
      "ACTION_HISTORY: CLICK\n",
      "ID: 5132\n",
      "ID: 5132\n",
      "ACTION_HISTORY: CLICK\n",
      "ID: 51327\n",
      "ID: 51327\n",
      "ACTION_HISTORY: CLICK\n",
      "ID: 48887\n",
      "ACTION_HISTORY: CLICK\n",
      "ID: 513\n",
      "Operation: CLICK\n",
      "Value: \n",
      "ID: 48887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 729/1030 [12:18<06:08,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation: CLICK\n",
      "Element: #38090\n",
      "Value: \n",
      "Action: CLICK\n",
      "Element: #38127\n",
      "Operation: TYPE\n",
      "Value: 1200\n",
      "ID: 38090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 783/1030 [13:13<07:17,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation: CLICK\n",
      "Action_HISTORY: \n",
      "TYPE: \n",
      "TYPE: \n",
      "TYPE: \n",
      "TYPE: \n",
      "TYPE: \n",
      "TYPE: \n",
      "TYPE: \n",
      "TYPE: \n",
      "TYPE: \n",
      "TYPE: \n",
      "TYPE: \n",
      "TYPE: \n",
      "TYPE: \n",
      "TYPE: \n",
      "TYPE: 0\n",
      "TYPE: 0\n",
      "TYPE: 0\n",
      "TYPE: 0\n",
      "TYPE: 0\n",
      "TYPE: 0\n",
      "TYPE: 0\n",
      "TYPE: 0\n",
      "TYPE: 0\n",
      "TYPE: 0\n",
      "TYPE: 0\n",
      "TYPE: 0\n",
      "TYPE:\n",
      "Operation: CLICK\n",
      "Value: \n",
      "ID: 38710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [17:32<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'step_sr': 0.4186952288218111, 'mean_id_distance': 0.04673762383723364, 'op_sr': 0.8919182083739046, 'mean_value_f1': 0.4998027255512285}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_dataset_names = [\"test_website\"]\n",
    "\n",
    "for test_dataset_name in test_dataset_names:\n",
    "    print(f\"testing {test_dataset_name} dpo dataset\")\n",
    "    results = []\n",
    "\n",
    "    with open(f\"/root/data/mind2web_dpo_{test_dataset_name}_ranked_50.json\", \"r\") as file:\n",
    "        test_dataset = json.load(file)\n",
    "\n",
    "    for dat in tqdm(test_dataset):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": dat[\"instruction\"]},\n",
    "            {\"role\": \"user\", \"content\": dat[\"input\"]},\n",
    "        ]\n",
    "        \n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "    \n",
    "        terminators = [\n",
    "            tokenizer.eos_token_id,\n",
    "            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=128,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        \n",
    "        response = outputs[0][input_ids.shape[-1]:]\n",
    "        output = tokenizer.decode(response, skip_special_tokens=True)\n",
    "        gt = dat[\"output\"]\n",
    "\n",
    "        try:\n",
    "            id_pattern = r\"ID: (\\d+)\"\n",
    "            output_id, gt_id = int(re.search(id_pattern, output).group(1)), int(re.search(id_pattern, gt).group(1))\n",
    "            \n",
    "            op_pattern = r\"Operation: (\\w+)\"\n",
    "            output_op, gt_op = re.search(op_pattern, output).group(1), re.search(op_pattern, gt).group(1)\n",
    "            \n",
    "            value_pattern = r\"Value: (\\w+)\"\n",
    "            gt_value = re.search(value_pattern, gt).group(1) if re.search(value_pattern, gt) else None\n",
    "            output_value = re.search(value_pattern, output).group(1) if re.search(value_pattern, output) else None\n",
    "        \n",
    "            value_f1 = None\n",
    "            if gt_op != \"CLICK\" and output_op != \"CLICK\" and (output_value and gt_value):\n",
    "                value_f1 = calculate_f1(output_value.lower(), gt_value.lower())\n",
    "            elif gt_op != \"CLICK\" and output_op != \"CLICK\" and not (output_value and gt_value):\n",
    "                value_f1 = 0.0\n",
    "            elif gt_op != \"CLICK\" and output_op == \"CLICK\":\n",
    "                value_f1 = 0.0\n",
    "            \n",
    "            result = {\n",
    "                \"step_sr\": gt.lower() == output.lower(), \n",
    "                \"id_distance\": abs(output_id - gt_id) / max(gt_id, output_id), \n",
    "                \"op_sr\": gt_op == output_op, \n",
    "                \"value_f1\": value_f1\n",
    "            }\n",
    "        \n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(output)\n",
    "            print(gt)\n",
    "            continue\n",
    "\n",
    "    with open(f\"/root/results/gemma_2b_it_50_dpo/mind2web_dpo_{test_dataset_name}_ranked_50.json\", \"w\") as file:\n",
    "        json.dump(results, file, indent=4)\n",
    "\n",
    "    step_sr = np.mean([result[\"step_sr\"] for result in results])\n",
    "    mean_id_distance = np.mean([result[\"id_distance\"] for result in results])\n",
    "    op_sr = np.mean([result[\"op_sr\"] for result in results])\n",
    "    mean_value_f1 = np.mean([result[\"value_f1\"] for result in [x for x in results if x[\"value_f1\"] != None]])\n",
    "    \n",
    "    mean_result = {\n",
    "        \"step_sr\": step_sr,\n",
    "        \"mean_id_distance\": mean_id_distance,\n",
    "        \"op_sr\": op_sr,\n",
    "        \"mean_value_f1\": mean_value_f1\n",
    "    }\n",
    "    \n",
    "    print(mean_result)\n",
    "    \n",
    "    with open(f\"/root/results/gemma_2b_it_50_dpo/mind2web_dpo_{test_dataset_name}_ranked_50_mean.json\", \"w\") as file:\n",
    "        json.dump(mean_result, file, indent=4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a5185b9-ee7f-471f-bbff-b12ff396f1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ccdf20-d76b-4908-8064-5de991a39839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
