{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35756fd2-7855-43df-91df-ba29bec992e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"autodl-tmp/gemma-2b-it\", trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"autodl-tmp/Meta-Llama-3-8B-Instruct\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1bd9790-c561-442c-98f9-0a9a354efbd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zero-shot\n",
    "instruction = ( \n",
    "    \"You are a web navigation intelligence who interacts with webpage environments to achieve human user intent.\\n\"\n",
    "    \"You always generate the next ACTION based on the user's INTENT, current cleaned webpage HTML and ACTION_HISTORY sequence which recording the actions that have been performed.\\n\\n\"\n",
    "    \"Given HTML and INTENT and ACTION_HISTORY, you should\\n\"\n",
    "    \"(1) Rely on your HTML code comprehension to analyze and understand what elements are on the current page.\\n\"\n",
    "    \"(2) Depend on your reasoning skills to parse the user's INTENT and infer the next action that should be taken in conjunction with the historical trajectory ACTION_HISTORY.\\n\"\n",
    "    \"(3) Select an element carefully from HTML code to interact with, thus bringing the goal closer to completion.\\n\\n\"\n",
    "    \"Your output format should be strictly as follows\\n\"\n",
    "    \"Operation: ... (should be CLICK or TYPE)\\n\"\n",
    "    \"Value: ... (optional textual value for the operation TYPE)\\n\"\n",
    "    \"ID: ... (unique id number for the element to click or type into)\\n\\n\"\n",
    "    # \"Thought: ... (A paragraph explaining why you chose this element to interact with, no more than 50 words)\"\n",
    "    \"Now, begin!\"\n",
    ")\n",
    "\n",
    "user_input_template = \"INTENT:\\n{intent}\\n\\nHTML:\\n{html}\\n\\nACTION_HISTORY:\\n{action_history}\"\n",
    "output_template = \"Operation: {op}\\nValue: {value}\\nID: {id}\"\n",
    "\n",
    "len(tokenizer(instruction)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "748d8013-f6fc-4e30-af88-56de6b8e1ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:25<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31 1989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:25<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 52 2106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:22<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 35 1533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:26<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 56 1929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:26<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 32 1818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:26<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 27 1875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:22<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 49 1854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:24<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 51 2073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:23<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 47 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:26<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 30 2007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:01<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 3 129\n",
      "too large token_num: 3\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import datasets\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lxml\n",
    "from lxml import etree\n",
    "from dom_utils import prune_tree\n",
    "\n",
    "train_dataset = []\n",
    "\n",
    "for ID in range(11):\n",
    "    with open(f\"autodl-tmp/train_dataset/train/train_{ID}.json\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    pos_candidate_na = 0\n",
    "    total_dataset_num = 0\n",
    "    large_token_num = 0\n",
    "    \n",
    "    for dat in tqdm(data):\n",
    "        intent = dat[\"confirmed_task\"] # + f\"(domain {dat['subdomain']})\"\n",
    "        action_history_all = dat[\"action_reprs\"]\n",
    "        annotation_id = dat['annotation_id']\n",
    "        \n",
    "        for index, d in enumerate(dat[\"actions\"]):\n",
    "            cleaned_html = d[\"cleaned_html\"]\n",
    "            action_uid = d['action_uid']\n",
    "            sample_id = f\"{annotation_id}_{action_uid}\"\n",
    "\n",
    "            if len(d[\"pos_candidates\"]) == 0:\n",
    "                pos_candidate_na += 1\n",
    "                continue \n",
    "                        \n",
    "            gt = d[\"pos_candidates\"][0][\"backend_node_id\"]\n",
    "            random.shuffle(d[\"neg_candidates\"])\n",
    "            neg_candidates_pool = d[\"neg_candidates\"][:20] # 10 or 50\n",
    "            candidate_ids = [gt] + [c[\"backend_node_id\"] for c in neg_candidates_pool]\n",
    "\n",
    "            dom_tree = lxml.etree.fromstring(cleaned_html)\n",
    "            dom_tree = prune_tree(dom_tree, candidate_ids)\n",
    "            html = lxml.etree.tostring(dom_tree, pretty_print=True, method=\"html\", encoding='unicode')\n",
    "            html = html.replace(\"backend_node_id\", \"id\")\n",
    "\n",
    "            action_history = action_history_all[:index]\n",
    "            token_num = len(tokenizer(html)[\"input_ids\"]) + len(tokenizer(intent)[\"input_ids\"]) + len(tokenizer(str(action_history))[\"input_ids\"]) + 20\n",
    "            \n",
    "            if token_num < 4000: # 8000\n",
    "                op = d[\"operation\"][\"op\"]\n",
    "                value = d[\"operation\"][\"value\"]\n",
    "                chosen_answer_ = output_template.format(op=op, value=value, id=d[\"pos_candidates\"][0][\"backend_node_id\"])\n",
    "                \n",
    "                random.shuffle(neg_candidates_pool)\n",
    "                rand_neg_candidates = neg_candidates_pool[:3]\n",
    "                \n",
    "                for c in rand_neg_candidates: # 1:3 proportion\n",
    "                    if op != \"CLICK\" and random.uniform(0, 1) < 0.33: # 1/3 for type/select -> click\n",
    "                        rejected_answer_ = output_template.format(op=\"CLICK\", value=\"\", id=c[\"backend_node_id\"])\n",
    "                    else:\n",
    "                        rejected_answer_ = output_template.format(op=op, value=value, id=c[\"backend_node_id\"])\n",
    "                        \n",
    "                    instruction_ = instruction\n",
    "                    input_ = user_input_template.format(intent=intent, html=html, action_history=action_history)\n",
    "                    output_ = [chosen_answer_, rejected_answer_]\n",
    "                    \n",
    "                    total_dataset_num += 1\n",
    "                    train_dataset.append({\n",
    "                        \"instruction\": instruction_,\n",
    "                        \"input\": input_,\n",
    "                        \"output\": output_\n",
    "                    })\n",
    "                    \n",
    "            else:\n",
    "                large_token_num += 1\n",
    "                # print(\"too large token_num:\", token_num)\n",
    "            \n",
    "    print(ID, pos_candidate_na, total_dataset_num)\n",
    "\n",
    "print(\"too large token_num:\", large_token_num)\n",
    "\n",
    "with open(\"/root/data/mind2web_dpo_train_50_gemma.json\", \"w\") as file:\n",
    "    json.dump(train_dataset, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d78e042-4cc9-42aa-904e-0741dab46cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/scores_all_data.pkl\", 'rb') as file:\n",
    "    scores = pickle.load(file)\n",
    "\n",
    "candidate_scores = scores[\"scores\"]\n",
    "candidate_ranks = scores[\"ranks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59e5c559-4799-40d6-b2b1-3d6b49db837b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating test_website dpo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:48<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 38 730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:35<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 21 484\n",
      "too large token_num: 49\n",
      "generating test_task dpo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:59<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 69 735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:50<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 34 709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:24<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 16 417\n",
      "too large token_num: 32\n",
      "generating test_domain dpo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:30<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 42 598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:30<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 25 632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:34<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 34 528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:35<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 51 671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:28<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 34 556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:23<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 35 524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:25<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 30 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:29<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 42 586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:27<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 26 645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:03<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 2 64\n",
      "too large token_num: 1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import datasets\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lxml\n",
    "from lxml import etree\n",
    "from dom_utils import prune_tree\n",
    "\n",
    "test_dataset_names = [\"test_website\", \"test_task\", \"test_domain\"]\n",
    "test_dataset_counts = [2, 3, 10]\n",
    "\n",
    "for test_dataset_name, test_dataset_count in zip(test_dataset_names, test_dataset_counts):\n",
    "    print(f\"generating {test_dataset_name} dpo dataset\")\n",
    "    test_dataset = []\n",
    "    \n",
    "    for ID in range(test_dataset_count):\n",
    "        with open(f\"autodl-tmp/test_dataset/{test_dataset_name}/{test_dataset_name}_{ID}.json\") as file:\n",
    "            data = json.load(file)\n",
    "    \n",
    "        pos_candidate_na = 0\n",
    "        total_dataset_num = 0\n",
    "        large_token_num = 0\n",
    "    \n",
    "        for dat in tqdm(data):\n",
    "            intent = dat[\"confirmed_task\"] # + f\"(domain {dat['subdomain']})\"\n",
    "            action_history_all = dat[\"action_reprs\"]\n",
    "            annotation_id = dat['annotation_id']\n",
    "        \n",
    "            for index, d in enumerate(dat[\"actions\"]):\n",
    "                cleaned_html = d[\"cleaned_html\"]\n",
    "                action_uid = d['action_uid']\n",
    "                sample_id = f\"{annotation_id}_{action_uid}\"\n",
    "    \n",
    "                if len(d[\"pos_candidates\"]) == 0:\n",
    "                    pos_candidate_na += 1\n",
    "                    continue\n",
    "\n",
    "                candidate_ids = []\n",
    "                for candidates in [d[\"pos_candidates\"], d[\"neg_candidates\"]]:\n",
    "                    for candidate in candidates:\n",
    "                        candidate_id = candidate[\"backend_node_id\"]\n",
    "                        rank = candidate_ranks[sample_id][candidate_id]\n",
    "                        if rank <= 50: # 10 or 50\n",
    "                            candidate_ids.append(candidate_id)\n",
    "\n",
    "                dom_tree = lxml.etree.fromstring(cleaned_html)\n",
    "                dom_tree = prune_tree(dom_tree, candidate_ids)\n",
    "                html = lxml.etree.tostring(dom_tree, pretty_print=True, method=\"html\", encoding='unicode')\n",
    "                html = html.replace(\"backend_node_id\", \"id\")\n",
    "    \n",
    "                action_history = action_history_all[:index]\n",
    "                token_num = len(tokenizer(html)[\"input_ids\"]) + len(tokenizer(intent)[\"input_ids\"]) + len(tokenizer(str(action_history))[\"input_ids\"]) + 20\n",
    "                \n",
    "                if token_num < 5000:\n",
    "                    op = d[\"operation\"][\"op\"]\n",
    "                    value = d[\"operation\"][\"value\"]\n",
    "                    chosen_answer_ = output_template.format(op=op, value=value, id=d[\"pos_candidates\"][0][\"backend_node_id\"])\n",
    "                    \n",
    "                    instruction_ = instruction\n",
    "                    input_ = user_input_template.format(intent=intent, html=html, action_history=action_history)\n",
    "                    output_ = chosen_answer_\n",
    "                    \n",
    "                    total_dataset_num += 1\n",
    "                    test_dataset.append({\n",
    "                        \"instruction\": instruction_,\n",
    "                        \"input\": input_,\n",
    "                        \"output\": output_\n",
    "                    })\n",
    "                    \n",
    "                else:\n",
    "                    large_token_num += 1\n",
    "    \n",
    "        print(ID, pos_candidate_na, total_dataset_num)\n",
    "\n",
    "    print(\"too large token_num:\", large_token_num)\n",
    "        \n",
    "    with open(f\"data/mind2web_dpo_{test_dataset_name}_ranked_50.json\", \"w\") as file:\n",
    "        json.dump(test_dataset, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c6ce63-54f1-424d-b735-5270d841f8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
